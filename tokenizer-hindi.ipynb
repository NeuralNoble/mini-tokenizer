{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5465585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus size: 50000 texts\n",
      "Total characters: 77,948,347\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "# Load datasets\n",
    "mc4 = load_dataset(\"zicsx/mC4-Hindi-Cleaned-2.0\", split=\"train\")\n",
    "stories = load_dataset(\"OmAlve/TinyStories-Hindi\", split=\"train\")\n",
    "\n",
    "# Sample 30k total: 18k mC4 + 12k stories (60-40 split)\n",
    "mc4_sample_size = 40_000\n",
    "stories_sample_size = 10_000\n",
    "\n",
    "# Random sampling\n",
    "random.seed(42)  # for reproducibility\n",
    "mc4_indices = random.sample(range(len(mc4)), mc4_sample_size)\n",
    "stories_indices = random.sample(range(len(stories)), stories_sample_size)\n",
    "\n",
    "mc4_sampled = mc4.select(mc4_indices)\n",
    "stories_sampled = stories.select(stories_indices)\n",
    "\n",
    "# Extract text\n",
    "mc4_texts = [row['text'] for row in mc4_sampled]\n",
    "stories_texts = [row['translated'] for row in stories_sampled]\n",
    "\n",
    "# Combine and shuffle\n",
    "all_texts = mc4_texts + stories_texts\n",
    "random.shuffle(all_texts)\n",
    "\n",
    "# Save corpus\n",
    "training_text = '\\n'.join(all_texts)\n",
    "\n",
    "with open('hindi_corpus_30k.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(training_text)\n",
    "\n",
    "print(f\"Corpus size: {len(all_texts)} texts\")\n",
    "print(f\"Total characters: {len(training_text):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02298a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "भारतीय वन डे टीम से लगभग एक साल से बहार चल रहे शानदार बल्लेबाज सुरेश रैना की वापसी पर फिर ग्रहण लग गया है। वायरल बुखार के चलते न्यूजीलैंड के खिलाफ चल रही सीरीज के पहले वनडे से बाहर होने के बाद रैना मंगलवार को टीम इंडिया से न सिर्फ जुड़ गए बल्कि उन्होंने हाथ में बल्ला भी उठा लिया।\n",
      "धोनी की दिक्कत यह होती कि धर्मशाल में अच्छा प्रदर्शन करने वाले केदार जाधव को रैना के लिए कैसे नज़र अंदाज़ किया जाए। फिलहाल दोनों के लिए कोई परेशानी खड़ी नहीं होने जा रही है। देर शाम बीसीसीआई की मेडिकल टीम ने साफ़ कर दिया कि \n"
     ]
    }
   ],
   "source": [
    "print(training_text[:500]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f63f6e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokens: ['भारतीय', ' वन', ' डे', ' टीम', ' से', ' लगभग', ' एक', ' साल', ' से', ' बहार', ' चल', ' रहे', ' शानदार', ' बल्लेबाज', ' सुरेश', ' रैना', ' की', ' वापसी', ' पर', ' फिर', ' ग्रहण', ' लग', ' गया', ' है', '।', ' वायरल', ' बुखार', ' के', ' चलते', ' न्यूजीलैंड', ' के', ' खिलाफ', ' चल', ' रही', ' सीरीज', ' के', ' पहले', ' वनडे', ' से', ' बाहर', ' होने', ' के', ' बाद', ' रैना', ' मंगलवार', ' को', ' टीम', ' इंडिया', ' से', ' न']\n",
      "Token count: 425\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "\n",
    "# Hindi-focused BPE pattern\n",
    "HINDI_PATTERN = re.compile(\n",
    "    r\"( ?[\\p{Devanagari}\\p{M}]+| ?\\p{N}+| ?[^\\p{Devanagari}\\p{M}\\p{N}\\s]+|\\s+)\",\n",
    "    re.UNICODE\n",
    ")\n",
    "\n",
    "def pretokenize_hindi(text):\n",
    "    return re.findall(HINDI_PATTERN, text)\n",
    "\n",
    "# Test on first 2000 chars\n",
    "tokens = pretokenize_hindi(training_text[:2000])\n",
    "print(\"Sample tokens:\", tokens[:50])\n",
    "print(\"Token count:\", len(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7365f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pre-tokens: 17498763\n",
      "Example token: ['भ', 'ा', 'र', 'त', 'ी', 'य']\n"
     ]
    }
   ],
   "source": [
    "# Build corpus tokens using characters instead of bytes\n",
    "corpus_tokens = []\n",
    "\n",
    "# Convert full big_text to pre-tokens\n",
    "ptoks = pretokenize_hindi(training_text)\n",
    "\n",
    "for pt in ptoks:\n",
    "    # list(pt) splits Hindi string into real Unicode chars\n",
    "    symbols = list(pt)\n",
    "    corpus_tokens.append(symbols)\n",
    "\n",
    "print(\"Total pre-tokens:\", len(corpus_tokens))\n",
    "print(\"Example token:\", corpus_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2af45662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique chars: 214\n",
      "Chars: ['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '¢', '¦', '©', 'Â', 'Ã', 'â', 'œ', 'ँ', 'ं', 'ः', 'अ', 'आ', 'इ', 'ई', 'उ', 'ऊ', 'ऋ', 'ऌ', 'ऍ', 'ऎ', 'ए', 'ऐ', 'ऑ', 'ऒ', 'ओ', 'औ', 'क', 'ख', 'ग', 'घ', 'ङ', 'च', 'छ', 'ज', 'झ', 'ञ', 'ट', 'ठ', 'ड', 'ढ', 'ण', 'त', 'थ', 'द', 'ध', 'न', 'ऩ', 'प', 'फ', 'ब', 'भ', 'म', 'य', 'र', 'ऱ', 'ल', 'ळ', 'ऴ', 'व', 'श', 'ष', 'स', 'ह', '़', 'ऽ', 'ा', 'ि', 'ी', 'ु', 'ू', 'ृ', 'ॄ', 'ॅ', 'ॆ', 'े', 'ै', 'ॉ', 'ॊ', 'ो', 'ौ', '्', 'ॎ', 'ॐ', '॑', '॒', '॓', '॔', 'क़', 'ख़', 'ग़', 'ज़', 'ड़', 'ढ़', 'फ़', 'य़', 'ॠ', 'ॢ', '।', '॥', '०', '१', '२', '३', '४', '५']\n",
      "First 20 id2sym: [(0, '\\n'), (1, ' '), (2, '!'), (3, '\"'), (4, '#'), (5, '$'), (6, '%'), (7, '&'), (8, \"'\"), (9, '('), (10, ')'), (11, '*'), (12, '+'), (13, ','), (14, '-'), (15, '.'), (16, '/'), (17, '0'), (18, '1'), (19, '2')]\n",
      "Example ids: [145, 160, 148, 136, 162, 147]\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: Build initial character vocabulary\n",
    "unique_chars = set()\n",
    "\n",
    "for tok in corpus_tokens:\n",
    "    for ch in tok:\n",
    "        unique_chars.add(ch)\n",
    "\n",
    "unique_chars = sorted(list(unique_chars))\n",
    "print(\"Number of unique chars:\", len(unique_chars))\n",
    "print(\"Chars:\", unique_chars[:200])  # just to inspect\n",
    "\n",
    "\n",
    "# STEP 3: Map chars to integer IDs\n",
    "sym2id = {}\n",
    "id2sym = {}\n",
    "\n",
    "for i, ch in enumerate(unique_chars):\n",
    "    sym2id[ch] = i\n",
    "    id2sym[i] = ch\n",
    "\n",
    "print(\"First 20 id2sym:\", [(i, id2sym[i]) for i in range(min(20, len(id2sym)))])\n",
    "\n",
    "\n",
    "# STEP 4: Convert corpus_tokens -> corpus_ids\n",
    "corpus_ids = []\n",
    "for tok in corpus_tokens:\n",
    "    ids = [sym2id[ch] for ch in tok]\n",
    "    corpus_ids.append(ids)\n",
    "\n",
    "print(\"Example ids:\", corpus_ids[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c5dc3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "def build_initial_stats(corpus_ids):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      global_pair_counts: Counter of all pairs across the corpus\n",
    "      pair_to_sequences: mapping pair -> set of sequence indices that contain it\n",
    "    \"\"\"\n",
    "    global_pair_counts = Counter()\n",
    "    pair_to_sequences = defaultdict(set)\n",
    "\n",
    "    for seq_id, seq in enumerate(corpus_ids):\n",
    "        for a, b in zip(seq, seq[1:]):\n",
    "            pair = (a, b)\n",
    "            global_pair_counts[pair] += 1\n",
    "            pair_to_sequences[pair].add(seq_id)\n",
    "\n",
    "    return global_pair_counts, pair_to_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "588bda53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pair(\n",
    "    a, b, new_id,\n",
    "    corpus_ids,\n",
    "    global_pair_counts,\n",
    "    pair_to_sequences\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform optimized BPE merge of pair (a, b) -> new_id.\n",
    "\n",
    "    Updates:\n",
    "      - corpus_ids\n",
    "      - global_pair_counts\n",
    "      - pair_to_sequences\n",
    "    Only sequences containing (a,b) are modified.\n",
    "    \"\"\"\n",
    "\n",
    "    # the pair we are merging\n",
    "    pair = (a, b)\n",
    "\n",
    "    # sequences that contain this pair\n",
    "    affected = pair_to_sequences.get(pair, set())\n",
    "\n",
    "    # we will delete this entry after merge\n",
    "    if pair in pair_to_sequences:\n",
    "        del pair_to_sequences[pair]\n",
    "    if pair in global_pair_counts:\n",
    "        del global_pair_counts[pair]\n",
    "\n",
    "    # iterate only those sequences\n",
    "    for seq_id in affected:\n",
    "        seq = corpus_ids[seq_id]\n",
    "\n",
    "        # --------------------------\n",
    "        # 1. REMOVE OLD PAIR COUNTS\n",
    "        # --------------------------\n",
    "        # remove all pairs from this sequence from global counts\n",
    "        for x, y in zip(seq, seq[1:]):\n",
    "            global_pair_counts[(x, y)] -= 1\n",
    "            pair_to_sequences[(x, y)].discard(seq_id)\n",
    "            if global_pair_counts[(x, y)] <= 0:\n",
    "                del global_pair_counts[(x, y)]\n",
    "                del pair_to_sequences[(x, y)]\n",
    "\n",
    "        # --------------------------\n",
    "        # 2. MERGE THE SEQUENCE\n",
    "        # --------------------------\n",
    "        new_seq = []\n",
    "        i = 0\n",
    "        L = len(seq)\n",
    "        while i < L:\n",
    "            if i < L - 1 and seq[i] == a and seq[i+1] == b:\n",
    "                new_seq.append(new_id)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_seq.append(seq[i])\n",
    "                i += 1\n",
    "\n",
    "        corpus_ids[seq_id] = new_seq\n",
    "\n",
    "        # --------------------------\n",
    "        # 3. ADD NEW PAIR COUNTS\n",
    "        # --------------------------\n",
    "        for x, y in zip(new_seq, new_seq[1:]):\n",
    "            global_pair_counts[(x, y)] += 1\n",
    "            pair_to_sequences[(x, y)].add(seq_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0be618",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_pair_counts, pair_to_sequences = build_initial_stats(corpus_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb03b9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] merge ' ' + 'क' -> ' क' (freq=2659325)\n",
      "[100] merge 'त' + '्' -> 'त्' (freq=84654)\n",
      "[200] merge 'ि' + 'र' -> 'िर' (freq=38373)\n",
      "[300] merge ' ' + '\"' -> ' \"' (freq=25154)\n",
      "[400] merge ' उन' + 'के' -> ' उनके' (freq=18503)\n",
      "[500] merge ' क' + '्यों' -> ' क्यों' (freq=14240)\n",
      "[600] merge 'े' + 'ड' -> 'ेड' (freq=11318)\n",
      "[700] merge 'ेश' + 'न' -> 'ेशन' (freq=9532)\n",
      "[800] merge ' ह' + 'ूं' -> ' हूं' (freq=8184)\n",
      "[900] merge ' ब' + 'ल्' -> ' बल्' (freq=7055)\n",
      "[1000] merge 'आ' + 'र' -> 'आर' (freq=6415)\n",
      "[1100] merge ' अ' + 'ंदर' -> ' अंदर' (freq=5669)\n",
      "[1200] merge 'क्' + 'शन' -> 'क्शन' (freq=5081)\n",
      "[1300] merge 'त्र' + 'ी' -> 'त्री' (freq=4630)\n",
      "[1400] merge ' इत' + 'ना' -> ' इतना' (freq=4257)\n",
      "[1500] merge 'ग' + 'म' -> 'गम' (freq=3878)\n",
      "[1600] merge 'ॉ' + 'ट' -> 'ॉट' (freq=3598)\n",
      "[1700] merge 'प' + 'े' -> 'पे' (freq=3311)\n",
      "[1800] merge ' पा' + 'या' -> ' पाया' (freq=3067)\n",
      "[1900] merge ' निक' + 'ाल' -> ' निकाल' (freq=2867)\n",
      "[2000] merge ' वाल' + 'ों' -> ' वालों' (freq=2677)\n",
      "[2100] merge ' ले' + 'ख' -> ' लेख' (freq=2521)\n",
      "[2200] merge ' नुक' + 'सान' -> ' नुकसान' (freq=2363)\n",
      "[2300] merge ' सा' + 'इ' -> ' साइ' (freq=2234)\n",
      "[2400] merge ' वरि' + 'ष्ठ' -> ' वरिष्ठ' (freq=2127)\n",
      "[2500] merge ' म' + 'ू' -> ' मू' (freq=1980)\n",
      "[2600] merge ' ट' + '्वी' -> ' ट्वी' (freq=1865)\n",
      "[2700] merge ' न' + 'ै' -> ' नै' (freq=1775)\n",
      "[2800] merge ' न' + 'ार' -> ' नार' (freq=1689)\n",
      "[2900] merge 'ब' + 'ॉ' -> 'बॉ' (freq=1613)\n",
      "[3000] merge ' जी' + 'व' -> ' जीव' (freq=1548)\n",
      "[3100] merge ' द' + 'या' -> ' दया' (freq=1474)\n",
      "[3200] merge ' पु' + 'स्तक' -> ' पुस्तक' (freq=1416)\n",
      "[3300] merge ' टे' + 'क्' -> ' टेक्' (freq=1355)\n",
      "[3400] merge 'ड' + 'िंग' -> 'डिंग' (freq=1298)\n",
      "[3500] merge ' संघ' + 'र्ष' -> ' संघर्ष' (freq=1249)\n",
      "[3600] merge ' ड' + 'ुअल' -> ' डुअल' (freq=1198)\n",
      "[3700] merge ' अद्' + 'भुत' -> ' अद्भुत' (freq=1150)\n",
      "[3800] merge ' आद' + 'ित्य' -> ' आदित्य' (freq=1106)\n",
      "[3900] merge ' ज' + 'ला' -> ' जला' (freq=1070)\n",
      "[4000] merge 'ाय' + 'ण' -> 'ायण' (freq=1033)\n",
      "[4100] merge ' स्ट' + 'ूड' -> ' स्टूड' (freq=1003)\n",
      "[4200] merge 'े' + 'न्द्र' -> 'ेन्द्र' (freq=967)\n",
      "[4300] merge ' अवै' + 'ध' -> ' अवैध' (freq=932)\n",
      "[4400] merge ' ह' + 'नु' -> ' हनु' (freq=894)\n",
      "[4500] merge ' फिल्म' + 'ें' -> ' फिल्में' (freq=866)\n",
      "[4600] merge ' फै' + 'ल' -> ' फैल' (freq=835)\n",
      "[4700] merge 'श' + 'ाह' -> 'शाह' (freq=806)\n",
      "[4800] merge ' जन' + 'पद' -> ' जनपद' (freq=782)\n",
      "[4900] merge ' तारी' + 'फ' -> ' तारीफ' (freq=760)\n",
      "[5000] merge 'ंग्ला' + 'देश' -> 'ंग्लादेश' (freq=744)\n",
      "[5100] merge 'स्' + 'वर' -> 'स्वर' (freq=720)\n",
      "[5200] merge ' तीस' + 'री' -> ' तीसरी' (freq=698)\n",
      "[5300] merge ' अना' + 'ज' -> ' अनाज' (freq=681)\n",
      "[5400] merge ' पौ' + 'धे' -> ' पौधे' (freq=664)\n",
      "[5500] merge ' स्तरी' + 'य' -> ' स्तरीय' (freq=646)\n",
      "[5600] merge ' ज्ञ' + 'ा' -> ' ज्ञा' (freq=630)\n",
      "[5700] merge 'स' + 'वाल' -> 'सवाल' (freq=614)\n",
      "[5800] merge '्र' + 'ल' -> '्रल' (freq=597)\n",
      "[5900] merge 'क्सी' + 'न' -> 'क्सीन' (freq=581)\n",
      "[6000] merge ' मांस' + 'पेश' -> ' मांसपेश' (freq=569)\n",
      "[6100] merge ' ह' + 'ंग' -> ' हंग' (freq=552)\n",
      "[6200] merge ' सह' + 'ारा' -> ' सहारा' (freq=540)\n",
      "[6300] merge ' शही' + 'द' -> ' शहीद' (freq=528)\n",
      "[6400] merge ' आर' + 'ंभ' -> ' आरंभ' (freq=516)\n",
      "[6500] merge 'ब' + 'ैक' -> 'बैक' (freq=505)\n",
      "[6600] merge 'प्र' + 'ो' -> 'प्रो' (freq=493)\n",
      "[6700] merge ' आ' + 'ंगन' -> ' आंगन' (freq=481)\n",
      "[6800] merge ' बे' + 'सिक' -> ' बेसिक' (freq=470)\n",
      "[6900] merge 'स्व' + 'ती' -> 'स्वती' (freq=458)\n",
      "[7000] merge ' उत्स' + 'व' -> ' उत्सव' (freq=448)\n",
      "[7100] merge ' प्रीम' + 'ियम' -> ' प्रीमियम' (freq=438)\n",
      "[7200] merge 'क्ष' + '्य' -> 'क्ष्य' (freq=427)\n",
      "[7300] merge ' आकर्ष' + 'ण' -> ' आकर्षण' (freq=418)\n",
      "[7400] merge ' वि' + 'डियो' -> ' विडियो' (freq=411)\n",
      "[7500] merge ' दिव्या' + 'ंग' -> ' दिव्यांग' (freq=401)\n",
      "[7600] merge ' उपा' + 'ध्याय' -> ' उपाध्याय' (freq=393)\n",
      "[7700] merge ' पात्र' + 'ता' -> ' पात्रता' (freq=385)\n",
      "[7800] merge ' करी' + 'बी' -> ' करीबी' (freq=378)\n",
      "[7900] merge ' ट्रॉ' + 'फी' -> ' ट्रॉफी' (freq=371)\n",
      "[8000] merge ' हार्म' + 'ोन' -> ' हार्मोन' (freq=364)\n",
      "[8100] merge 'संच' + 'ार' -> 'संचार' (freq=356)\n",
      "[8200] merge 'ु' + 'श' -> 'ुश' (freq=348)\n",
      "[8300] merge ' भ' + 'टक' -> ' भटक' (freq=343)\n",
      "[8400] merge ' अव' + 'काश' -> ' अवकाश' (freq=336)\n",
      "[8500] merge ' आ' + 'इड' -> ' आइड' (freq=329)\n",
      "[8600] merge 'दाय' + 'ी' -> 'दायी' (freq=323)\n",
      "[8700] merge ' डिप' + '्टी' -> ' डिप्टी' (freq=317)\n",
      "[8800] merge ' दिला' + 'ई' -> ' दिलाई' (freq=311)\n",
      "[8900] merge 'वि' + 'द' -> 'विद' (freq=305)\n",
      "[9000] merge 'ता' + 'ंत्रिक' -> 'तांत्रिक' (freq=300)\n",
      "[9100] merge ' क्रिया' + 'न्वयन' -> ' क्रियान्वयन' (freq=295)\n",
      "[9200] merge ' ऊंच' + 'ा' -> ' ऊंचा' (freq=289)\n",
      "[9300] merge ' छा' + 'प' -> ' छाप' (freq=284)\n",
      "[9400] merge ' ध' + 'केल' -> ' धकेल' (freq=280)\n",
      "[9500] merge ' उप' + 'रोक्त' -> ' उपरोक्त' (freq=275)\n",
      "[9600] merge ' सुंदर' + 'ता' -> ' सुंदरता' (freq=269)\n",
      "[9700] merge ' एकत्र' + 'ित' -> ' एकत्रित' (freq=265)\n",
      "[9800] merge ' राम' + 'लीला' -> ' रामलीला' (freq=260)\n",
      "[9900] merge ' २००' + '५' -> ' २००५' (freq=255)\n",
      "[10000] merge ' अंस' + 'ारी' -> ' अंसारी' (freq=251)\n",
      "[10100] merge ' स' + 'ान' -> ' सान' (freq=246)\n",
      "[10200] merge ' सु' + 'षमा' -> ' सुषमा' (freq=243)\n",
      "[10300] merge ' स्क्रीन' + 'िंग' -> ' स्क्रीनिंग' (freq=239)\n",
      "[10400] merge ' मी' + 'णा' -> ' मीणा' (freq=235)\n",
      "[10500] merge ' स' + 'चे' -> ' सचे' (freq=231)\n",
      "[10600] merge ' सरकार' + 'ें' -> ' सरकारें' (freq=228)\n",
      "[10700] merge ' र' + 'ॉक' -> ' रॉक' (freq=225)\n",
      "[10800] merge ' च' + 'ंचल' -> ' चंचल' (freq=222)\n",
      "[10900] merge ' पिक' + 'निक' -> ' पिकनिक' (freq=218)\n",
      "[11000] merge 'हर' + 'ण' -> 'हरण' (freq=215)\n",
      "[11100] merge ' रणनी' + 'तिक' -> ' रणनीतिक' (freq=212)\n",
      "[11200] merge ' बन' + 'ें' -> ' बनें' (freq=208)\n",
      "[11300] merge 'हम' + 'ारी' -> 'हमारी' (freq=205)\n",
      "[11400] merge ' सोल' + 'र' -> ' सोलर' (freq=202)\n",
      "[11500] merge ' गाय' + 'न' -> ' गायन' (freq=199)\n",
      "[11600] merge ' सू' + 'ख' -> ' सूख' (freq=196)\n",
      "[11700] merge ' बख' + 'ूबी' -> ' बखूबी' (freq=194)\n",
      "Done.\n",
      "Final vocab size: 12000\n"
     ]
    }
   ],
   "source": [
    "vocab_size_target = 12000\n",
    "initial_vocab_size = len(sym2id)\n",
    "next_token_id = initial_vocab_size\n",
    "\n",
    "merges = []\n",
    "\n",
    "for it in range(vocab_size_target - initial_vocab_size):\n",
    "\n",
    "    if not global_pair_counts:\n",
    "        print(\"No more pairs left to merge.\")\n",
    "        break\n",
    "\n",
    "    # pick best pair\n",
    "    (a, b), freq = global_pair_counts.most_common(1)[0]\n",
    "\n",
    "    if freq < 2:\n",
    "        print(\"Stopping early, freq < 2\")\n",
    "        break\n",
    "\n",
    "    new_id = next_token_id\n",
    "    next_token_id += 1\n",
    "\n",
    "    # build new symbol for logging\n",
    "    new_sym = id2sym[a] + id2sym[b]\n",
    "    id2sym[new_id] = new_sym\n",
    "    merges.append((a, b, new_id))\n",
    "\n",
    "    # log\n",
    "    if it % 100 == 0:\n",
    "        print(f\"[{it}] merge '{id2sym[a]}' + '{id2sym[b]}' -> '{new_sym}' (freq={freq})\")\n",
    "\n",
    "    # perform merge + update stats\n",
    "    merge_pair(\n",
    "        a, b, new_id,\n",
    "        corpus_ids,\n",
    "        global_pair_counts,\n",
    "        pair_to_sequences\n",
    "    )\n",
    "\n",
    "print(\"Done.\")\n",
    "print(f\"Final vocab size: {next_token_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbf59d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranks[(a,b)] = rank_index\n",
    "# lower rank means earlier / higher priority merge\n",
    "merge_ranks = {}\n",
    "for rank, (a, b, new_id) in enumerate(merges):\n",
    "    merge_ranks[(a, b)] = rank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "536ea0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_merges_to_sequence(seq):\n",
    "    while True:\n",
    "        if len(seq) < 2:\n",
    "            return seq\n",
    "        \n",
    "        pairs = [(seq[i], seq[i+1]) for i in range(len(seq)-1)]\n",
    "        ranked = [(merge_ranks[p], i, p) for i, p in enumerate(pairs) if p in merge_ranks]\n",
    "        \n",
    "        if not ranked:\n",
    "            return seq\n",
    "        \n",
    "        _, idx, (a, b) = min(ranked)\n",
    "        \n",
    "        # find new_id\n",
    "        for x, y, nid in merges:\n",
    "            if x == a and y == b:\n",
    "                new_id = nid\n",
    "                break\n",
    "        \n",
    "        new_seq = []\n",
    "        i = 0\n",
    "        while i < len(seq):\n",
    "            if i == idx:\n",
    "                new_seq.append(new_id)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_seq.append(seq[i])\n",
    "                i += 1\n",
    "        \n",
    "        seq = new_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "edfd5945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text):\n",
    "    tokens = pretokenize_hindi(text)\n",
    "    output_ids = []\n",
    "\n",
    "    for tok in tokens:\n",
    "        seq = [sym2id[ch] for ch in tok]\n",
    "        seq = apply_merges_to_sequence(seq)\n",
    "        output_ids.extend(seq)\n",
    "\n",
    "    return output_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3c29db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2755, 232, 839, 639, 192]\n",
      "Decoded: भारत में चुनाव हुआ।\n"
     ]
    }
   ],
   "source": [
    "text = \"भारत में चुनाव हुआ।\"\n",
    "ids = encode(text)\n",
    "print(ids)\n",
    "print(\"Decoded:\", \"\".join(id2sym[i] for i in ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b8516297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_vocab_json(path=\"vocab.json\"):\n",
    "    vocab = {}\n",
    "    for i, sym in id2sym.items():\n",
    "        vocab[sym] = i\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(vocab, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "save_vocab_json(\"vocab.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dfd945b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_merges_txt(path=\"merges.txt\"):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"#version: BPE\\n\")\n",
    "        for (a, b, new_id) in merges:\n",
    "            A = id2sym[a]\n",
    "            B = id2sym[b]\n",
    "            # Save raw symbols exactly how BPE expects\n",
    "            f.write(f\"{A} {B}\\n\")\n",
    "\n",
    "save_merges_txt(\"merges.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f7e63780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# You already have this from before:\n",
    "import regex as re\n",
    "HINDI_PATTERN = re.compile(\n",
    "    r\"( ?[\\p{Devanagari}\\p{M}]+| ?\\p{N}+| ?[^\\p{Devanagari}\\p{M}\\p{N}\\s]+|\\s+)\",\n",
    "    re.UNICODE\n",
    ")\n",
    "\n",
    "def pretokenize_hindi(text):\n",
    "    return re.findall(HINDI_PATTERN, text)\n",
    "\n",
    "\n",
    "class HindiBPETokenizer:\n",
    "    def __init__(self, id2sym, sym2id, merges):\n",
    "        self.id2sym = id2sym\n",
    "        self.sym2id = sym2id\n",
    "        self.merges = merges\n",
    "        \n",
    "        # build merge_ranks\n",
    "        self.merge_ranks = {\n",
    "            (a, b): rank for rank, (a, b, new_id) in enumerate(merges)\n",
    "        }\n",
    "\n",
    "        # Optional: direct map pair → new_id (for faster merges)\n",
    "        self.pair2newid = {(a, b): new_id for (a, b, new_id) in merges}\n",
    "\n",
    "    # -------------------------\n",
    "    # APPLY MERGES TO ONE SEQUENCE\n",
    "    # -------------------------\n",
    "    def _apply_merges(self, seq):\n",
    "        # Greedy BPE merge\n",
    "        while True:\n",
    "            if len(seq) < 2:\n",
    "                return seq\n",
    "\n",
    "            pairs = [(seq[i], seq[i+1]) for i in range(len(seq)-1)]\n",
    "\n",
    "            # find mergeable pairs\n",
    "            ranked = [\n",
    "                (self.merge_ranks[pair], i, pair)\n",
    "                for i, pair in enumerate(pairs)\n",
    "                if pair in self.merge_ranks\n",
    "            ]\n",
    "\n",
    "            if not ranked:\n",
    "                return seq\n",
    "\n",
    "            # choose highest priority merge (lowest rank)\n",
    "            _, idx, (a, b) = min(ranked)\n",
    "\n",
    "            new_id = self.pair2newid[(a, b)]\n",
    "\n",
    "            # apply it\n",
    "            new_seq = []\n",
    "            i = 0\n",
    "            L = len(seq)\n",
    "            while i < L:\n",
    "                if i == idx:\n",
    "                    new_seq.append(new_id)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_seq.append(seq[i])\n",
    "                    i += 1\n",
    "            seq = new_seq\n",
    "\n",
    "    # -------------------------\n",
    "    # ENCODE\n",
    "    # -------------------------\n",
    "    def encode(self, text):\n",
    "        pretoks = pretokenize_hindi(text)\n",
    "        output_ids = []\n",
    "\n",
    "        for tok in pretoks:\n",
    "            # split into chars → ids\n",
    "            seq = [self.sym2id[ch] for ch in tok]\n",
    "            # apply BPE\n",
    "            seq = self._apply_merges(seq)\n",
    "            # append\n",
    "            output_ids.extend(seq)\n",
    "\n",
    "        return output_ids\n",
    "\n",
    "    # -------------------------\n",
    "    # DECODE\n",
    "    # -------------------------\n",
    "    def decode(self, ids):\n",
    "        return \"\".join(self.id2sym[i] for i in ids)\n",
    "\n",
    "    # -------------------------\n",
    "    # SAVE\n",
    "    # -------------------------\n",
    "    def save(self, path=\"hindi_tokenizer.pkl\"):\n",
    "        data = {\n",
    "            \"id2sym\": self.id2sym,\n",
    "            \"sym2id\": self.sym2id,\n",
    "            \"merges\": self.merges\n",
    "        }\n",
    "        with open(path, \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "\n",
    "    # -------------------------\n",
    "    # LOAD\n",
    "    # -------------------------\n",
    "    @classmethod\n",
    "    def load(cls, path=\"hindi_tokenizer.pkl\"):\n",
    "        with open(path, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        return cls(\n",
    "            data[\"id2sym\"],\n",
    "            data[\"sym2id\"],\n",
    "            data[\"merges\"]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7fc8ef4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = HindiBPETokenizer(id2sym, sym2id, merges)\n",
    "tok.save(\"hindi_tokenizer.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "57a8dec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = HindiBPETokenizer.load(\"hindi_tokenizer.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ede16d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2755, 232, 839, 639, 192]\n"
     ]
    }
   ],
   "source": [
    "ids = tok.encode(\"भारत में चुनाव हुआ।\")\n",
    "print(ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "888e44e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "भारत में चुनाव हुआ।\n"
     ]
    }
   ],
   "source": [
    "print(tok.decode(ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a008e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_ratio = len(training_text)/len(tok.encode(training_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f180477f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.86336353891803\n"
     ]
    }
   ],
   "source": [
    "print(compression_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922cd11f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
